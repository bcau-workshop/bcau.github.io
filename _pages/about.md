---
layout: about
title: About
permalink: /

banner: /assets/img/logos/logo-bcau-wide.svg  # link to banner image relative to root
news: true  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
---

# Becoming Certain About Uncertainty (BCAU)

The value of models which output some notion of uncertainty is often seen as a fundamental fact. Little attention is given to what kind of uncertainty is being estimated and what the value of its quantification is. In this workshop we invite submissions which demonstrate that a model or algorithm outputting a notion of uncertainty has downstream application for its uncertainty estimate. Or works which show that the quantification of certain kinds of uncertainty does not help with a task that is often given for its motivation. We will also consider papers which propose a taxonomy of forms of uncertainty, or novel evaluation metrics, as long as they are grounded in a downstream application. We hope to stimulate a discussion around what notions of uncertainty there are, what their uses are and how they relate and integrate.


## Important Dates

<div>
<table>
    <tbody>
    <tr>
        <td><b>Submission Deadline</b></td>
        <td>September 22nd, 2022 07:00 AM UTC</td>
    </tr>
    <tr>
        <td><b>Workshop Accept / Reject Notification Date</b></td>
        <td>October 20th, 2022 07:00 AM UTC</td>
    </tr>
    <tr>
        <td><b>Workshop Date</b></td>
        <td>December 2nd or 3rd, 2022</td>
    </tr>
    </tbody>
</table>
</div>

## Call for Papers

We invite high-quality extended abstract submissions on different kinds of uncertainty quantification and their downstream application. Some examples (non-exhaustive list): 

- Downstream applications of uncertainty quantification in machine learning
- Notions of uncertainty and their taxonomy
- Novel or underappreciated notions of uncertainty (e.g. computational uncertainty)
- Evaluation metrics for uncertainty quantification
- Failure cases illustrating where uncertainty quantification is essential

### Submissions

Accepted submissions will be presented during joint poster sessions and will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals. 

Submissions must be anonymous, in <span style="color:#3a92d6;font-weight:400;"><a href="https://neurips.cc/Conferences/2022/PaperInformation/StyleFiles">NeurIPS format</a></span> and not longer than 4 pages excluding references, acknowledgements, and supplementary material. Long appendices are permitted but strongly discouraged, and reviewers are not required to read them. The review process is double-blind.

We also welcome submissions of recently published work that is strongly within the scope of the workshop (with proper formatting). We encourage the authors of such submissions to focus on accessibility to the wider NeurIPS community while distilling their work into an extended abstract. 

Authors may be asked to review other workshop submissions. 

## Schedule

Coming Soon!

<!-- 
<div>
<p><b>(EST) Morning </b></p>
<ul>
    <li>06:45 : Introduction and opening remarks</li>
    <li>07:00 : Invited Talk 1 - Weinan E - Machine Learning and PDEs</li>
    <li>07:45 : Spotlight Talk 1 - NeurInt-Learning Interpolation by Neural ODEs </li>
    <li>08:00 : Spotlight Talk 2 - Neural ODE Processes: A Short Summary  </li>
    <li>08:15 : Invited Talk 2 - Neha Yadav - Deep learning methods for solving differential equations </li>
    <li>09:00 : Coffee Break</li>
    <li>09:15 : Spotlight Talk 3 - GRAND: Graph Neural Diffusion </li>
    <li>09:30 : Spotlight Talk 4 - Neural Solvers for Fast and Accurate Numerical Optimal Control </li>
    <li>09:45 : Poster Session 1 - <a href="https://eventhosts.gather.town/wR7m2n0AMEhpzwM2/neurips2021reserveSpace8">GatherTown room</a></li>
    <li>10:30 : Invited Talk 3 - Philipp Grohs - The Theory-to-Practice Gap in Deep Learning </li>
    <li>11:15 : Lunch Break                </li>
</ul>
<p><b>(EST) Afternoon </b></p>
<ul>
    <li>13:45 : Spotlight Talk 5 - Deep Reinforcement Learning for Online Control of Stochastic Partial Differential Equations </li>
    <li>14:00 : Spotlight Talk 6 - Statistical Numerical PDE : Fast Rate, Neural Scaling Law and When it’s Optimal </li>
    <li>14:15 : Coffee Break</li>
    <li>14:30 : Poster Session 2 - <a href="https://eventhosts.gather.town/wR7m2n0AMEhpzwM2/neurips2021reserveSpace8">GatherTown room</a></li>
    <li>15:15 : Invited Talk 4 - Anima Anandkumar - Neural operator: A new paradigm for learning PDEs </li>
    <li>16:00 : Spotlight Talk 7 - HyperPINN: Learning parameterized differential equations with physics-informed hypernetworks  </li>
    <li>16:15 : Spotlight Talk 8 - Learning Implicit PDE Integration with Linear Implicit Layers </li>
</ul>
<p><b>(EST) Night </b></p>
<ul>
    <li>23:00 : Panel discussion - Solving Differential Equations with Deep Learning: State of the Art and Future Directions     </li>
    <li>24:00 : Final Remarks</li>
</ul>
</div> 
-->


## Invited Speakers


<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:30px;" src="{{ "/assets/img/people/tamara_broderick.jpg" | prepend:site.baseurl }}">
    <p><a href="https://tamarabroderick.com/"><b>Tamara Broderick</b></a>
    (confirmed) is an Associate Professor in the Department of Electrical Engineering and Computer Science at MIT. Her recent research has focused on developing and analyzing models for scalable Bayesian machine learning. She is interested in understanding how we can reliably quantify uncertainty and robustness in modern, complex data analysis procedures. To that end, she is particularly focused on Bayesian inference and graphical models – with an emphasis on scalable, nonparametric, and unsupervised learning. She has been awarded selection to the COPSS Leadership Academy (2021), an Early Career Grant (ECG) from the Office of Naval Research (2020), an NSF CAREER Award (2018), a Sloan Research Fellowship (2018), an Army Research Office Young Investigator Program (YIP) award (2017), Google Faculty Research Awards, an Amazon Research Award and the ISBA Lifetime Members Junior Researcher Award among others.</p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:30px;" src="{{ "/assets/img/people/philipp_hennig.jpg" | prepend:site.baseurl }}">
    <p><a href="https://uni-tuebingen.de/de/134782"><b>Philipp Hennig</b></a>
    (confirmed) holds the Chair for the Methods of Machine Learning at the University of Tübingen, and is an adjunct scientist at the Max Planck Institute for Intelligent Systems. He studied Physics in Heidelberg, Germany and at Imperial College, London, before moving to the University of Cambridge, UK, where he attained a PhD in the group of Sir David JC MacKay with research on machine learning. Since this time, he is interested in connections between computation and inference. With international collaborators, he helped establish the field of probabilistic numerics. His research was supported, among others, by the Emmy Noether Programme of the German Research Union (DFG), an independent Research Group of the Max Planck Society, and a Starting Grant of the European Commission. 
    <!-- Hennig is co-speaker of the Cyber Valley Initiative (with Michael Black and Thomas Kropf); Co-Director of the ELLIS Program on Theory, Algorithms and Computations of Modern Learning Systems (with Francis Bach and Lorenzo Rosasco), and Member of the Center of Excellence for Machine Learning in Science. He is a Co-PI of the IMPRS for Intelligent Systems and the Competence Center for Machine Learning in Tübingen. In 2019, he received the annual Award for Excellence in Teaching of the Union of CS Students. -->
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:30px;" src="{{ "/assets/img/people/balaji_lakshminarayanan.jpg" | prepend:site.baseurl }}">
    <p><a href="http://www.gatsby.ucl.ac.uk/~balaji/"><b>Balaji Lakshminarayanan</b></a>
    (confirmed) is a Staff Research Scientist (Tech Lead, Manager) at Google Brain in Mountain View (USA), where he leads a team of research scientists and engineers. Prior to that, he was a Staff Research Scientist at DeepMind. Balaji's research interests are in scalable, probabilistic machine learning. His PhD thesis was focused on exploring (and exploiting) connections between neat mathematical ideas in (non-parametric) Bayesian land and computationally efficient tricks in decision tree land, to get the best of both worlds. More recently, he has focused on probabilistic deep learning, including but not limited to (out-of-distribution) robustness, deep generative models, normalizing flows and variational autoencoders, as well as applying probabilistic deep learning ideas in healthcare and Google products.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:30px;" src="{{ "/assets/img/people/yingzhen_li.jpg" | prepend:site.baseurl }}">
    <p><a href="http://yingzhenli.net/home/en/"><b>Yingzhen Li</b></a>
    (confirmed) is a lecturer at the Department of Computing at Imperial College London. Before that she spent 2.5 years as a senior researcher at Microsoft Research Cambridge. Yingzhen is interested in building reliable machine learning systems which can generalise to unseen environments. She approaches this goal using probabilistic modelling and representation learning. Her research topics include (deep) probabilistic graphical model design, fast and accurate (Bayesian) inference / computation techniques, uncertainty quantification for computation and downstream tasks, as well as robust and adaptive machine learning systems.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:30px;" src="{{ "/assets/img/people/andrew_gordon_wilson.jpg" | prepend:site.baseurl }}">
    <p><a href="https://cims.nyu.edu/~andrewgw/"><b>Andrew Gordon Wilson</b></a>
    (confirmed) is an Assistant Professor at the Courant Institute of Mathematical Sciences and the Center for Data Science at New York University. His research focuses on developing flexible, interpretable, and scalable machine learning models, often involving deep learning, Gaussian processes, and kernel learning. He cares about developing practically impactful methods, while at the same understanding why the methods work, and the foundations for building models that learn and generalize. Andrew is particularly excited about loss surfaces, generalization, probabilistic generative models, physics inspired methods, and Bayesian methods in deep learning. His work has been applied to time series, vision, NLP, spatial statistics, public policy, medicine, and physics.
    </p>
  </div>
</div>


## Organizers

<div class="organizers" style="  display: flex;  flex-wrap: wrap;  text-align: center;  gap: 30px;">

   <div class="col-xs-3">
    <a href="https://scholar.google.com/citations?user=zQsc0mcAAAAJ&hl=en">
      <img class="people-pic" src="{{ "/assets/img/people/taiga_abe.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://scholar.google.com/citations?user=zQsc0mcAAAAJ&hl=en">Taiga Abe</a>
      <h6>Columbia University</h6>
    </div>
  </div>

 <div class="col-xs-3">
    <a href="https://banerjee-arundhati.github.io/">
      <img class="people-pic" src="{{ "/assets/img/people/arundhati_banerjee.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://banerjee-arundhati.github.io/">Arundhati Banerjee</a>
      <h6>Carnegie Mellon University</h6>
    </div>
  </div>

 <div class="col-xs-3">
    <a href="https://www.ekbuchanan.com/">
      <img class="people-pic" src="{{ "/assets/img/people/kelly_buchanan.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ekbuchanan.com/">Kelly Buchanan</a>
      <h6>Columbia University</h6>
    </div>
  </div>

 <div class="col-xs-3">
    <a href="https://polkirichenko.github.io/">
      <img class="people-pic" src="{{ "/assets/img/people/polina_kirichenko.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://polkirichenko.github.io/">Polina Kirichenko</a>
      <h6>New York University</h6>
    </div>
  </div>

 <div class="col-xs-3">
    <a href="https://sflippl.github.io/">
      <img class="people-pic" src="{{ "/assets/img/people/samuel_lippl.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://sflippl.github.io/">Samuel Lippl</a>
      <h6>Columbia University</h6>
    </div>
  </div>

 <div class="col-xs-3">
    <a href="https://geoffpleiss.com/">
      <img class="people-pic" src="{{ "/assets/img/people/geoff_pleiss.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://geoffpleiss.com/">Geoff Pleiss</a>
      <h6>Columbia University</h6>
    </div>
  </div>

 <div class="col-xs-3">
    <a href="https://www.linht.com/">
      <img class="people-pic" src="{{ "/assets/img/people/linh_tran.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linht.com/">Linh Tran</a>
      <h6>AutoDesk AI lab</h6>
    </div>
  </div>

   <div class="col-xs-3">
    <a href="https://jonathanwenger.netlify.app/">
      <img class="people-pic" src="{{ "/assets/img/people/jonathan_wenger.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://jonathanwenger.netlify.app/">Jonathan Wenger</a>
      <h6>University of Tübingen</h6>
    </div>
  </div>

   <div class="col-xs-3">
    <a href="https://luhuanwu.github.io/">
      <img class="people-pic" src="{{ "/assets/img/people/luhuan_wu.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://luhuanwu.github.io/">Luhuan Wu</a>
      <h6>Columbia University</h6>
    </div>
  </div>

</div>